<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://www.viviano.ca/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.viviano.ca/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-17T06:36:59+00:00</updated><id>https://www.viviano.ca/feed.xml</id><title type="html">Mode Collapse</title><subtitle>. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://www.viviano.ca/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://www.viviano.ca/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://www.viviano.ca/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://www.viviano.ca/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://www.viviano.ca/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://www.viviano.ca/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">You Can’t Understand A Deep Learning Model by Watching Where It’s Looking</title><link href="https://www.viviano.ca/blog/2021/saliency/" rel="alternate" type="text/html" title="You Can’t Understand A Deep Learning Model by Watching Where It’s Looking"/><published>2021-01-24T10:32:20+00:00</published><updated>2021-01-24T10:32:20+00:00</updated><id>https://www.viviano.ca/blog/2021/saliency</id><content type="html" xml:base="https://www.viviano.ca/blog/2021/saliency/"><![CDATA[<p>Deep learning’s promise in medical applications are obvious, but it will be hard to get wide acceptance (or regulatory approvals) without doctors being able to understand the “why” behind each prediction. A popular method for doing so is looking at a model’s “saliency” map, which looks like a weather map on top of the image used to make the prediction. The brightest areas are believed to be the regions most involved in the prediction.</p> <p>This method led to some concern. Take this example, from <a href="https://arxiv.org/abs/1807.00431">this excellent paper by John R. Zech et al</a>. In it, we can see that the model is using a metal token that radiology technicians place on the patient (top right hand corner) during image capture to make the prediction (the token helps Radiologists distinguish left from right later on). The issue is when the model notices that people who have a particular kind of token are also more likely to have some disease, say pneumonia, or cardiomegaly (enlarged heart), the model will seem to use the token when making the prediction.</p> <p style="text-align: center;"><img src="/assets/img/saliency/bad-saliency.png" alt="Bad Saliency"/></p> <p>These kinds of relationships are found everywhere, and called “spurious correlations”. The problem with spurious correlations is you can’t rely on them… they might be very high in one dataset, but very low in another, because there is no reason for the two variables to be related at all, and the original correlation was just due to bad luck. Humans are generally good at detecting and discarding these kinds of correlations when they are simple, but very bad at discarding them when the real relationships aren’t immediately apparent. For example, most people would discard <a href="https://www.tylervigen.com/spurious-correlations">this correlation</a> as spurious:</p> <p style="text-align: center;"><img src="/assets/img/saliency/bad-correlation.png" alt="Bad Correlation"/></p> <p>but people often fall for experts who previously predicted some impossible-to-predict event, ignoring the fact that thousands of people were making wrong predictions at the same time and it is possible that this <a href="https://hbr.org/2013/04/experts-who-beat-the-odds-are-probably-just-lucky">expert might have just been lucky</a>. Very common in finance.</p> <p>This project was inspired by the above problem. We know where doctors want the models to look for disease, and we can see where the model is looking, so maybe we can directly teach the model to look in the right place, and show that this helps the model generalize to new situations better.</p> <p>I’m ecstatic that my first publication in a “real” machine learning venue was the <a href="https://iclr.cc/">International Conference on Learning Representations (ICLR)</a>. I’ll summarize the <a href="https://openreview.net/forum?id=c9-WeM-ceB">paper</a> soon. :)</p>]]></content><author><name>josephdviviano</name></author><category term="Publications"/><summary type="html"><![CDATA[Deep learning’s promise in medical applications are obvious, but it will be hard to get wide acceptance (or regulatory approvals) without doctors being able to understand the “why” behind each prediction. A popular method for doing so is looking at a model’s “saliency” map, which looks like a weather map on top of the image used to make the prediction. The brightest areas are believed to be the regions most involved in the prediction.]]></summary></entry><entry><title type="html">Redefining Psychiatric Treatment Using Machine Learning</title><link href="https://www.viviano.ca/blog/2018/biomarkers/" rel="alternate" type="text/html" title="Redefining Psychiatric Treatment Using Machine Learning"/><published>2018-09-14T10:32:20+00:00</published><updated>2018-09-14T10:32:20+00:00</updated><id>https://www.viviano.ca/blog/2018/biomarkers</id><content type="html" xml:base="https://www.viviano.ca/blog/2018/biomarkers/"><![CDATA[<p>Machine learning can be used to find neurologically-distinct subtypes of Schizophrenia.</p> <p>A major issue with psychiatry is that individuals with the same diagnostic label from the <a href="https://www.psychiatry.org/psychiatrists/practice/dsm">Diagnostic and Statistical Manual of Mental Disorders (DSM)</a> can actually have very different symptoms or deficits: it is common to count symptoms, and then give a diagnosis if the patient reaches passes a “symptom threshold” (this is an oversimplification, but suffices for illustration). You can imagine that using this scheme, two individuals with the exact same diagnosis might have very few overlapping, or in extreme cases, no overlapping symptoms. This is particularly true with one of the most heterogeneous disorders: Schizophrenia.</p> <p>Schizophrenia has many subtypes, and it <a href="https://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.2014.14040435">isn’t clear that it is one disease</a>. Simultaneously with the project on predicting disease trajectory for those at risk of developing Alzheimer’s disease, I was helping the lab manage the <a href="https://academic.oup.com/schizophreniabulletin/article/45/Supplement_2/S144/5434549">Social Processes Initiative in Neurobiology of the Schizophrenia’s (SPINS)</a> study. During my 4 years at the <a href="https://www.camh.ca/en/science-and-research/institutes-and-centres/campbell-family-mental-health-research-institute/research-focus-campbell-institute/kimel-family-translational-imaging-genetics-lab">Centre for Addiction and Mental Health</a>, I spent a lot of my time managing the MRI and clinical data collected by the the three hospitals involved in the study. Briefly, the study was designed to explore the neurological underpinnings of social processing deficits seen in some subtypes of schizophrenia. There’s some evidence that the best treatment for Schizophrenia depends on the symptoms, or the subtype, and the optimal treatment varies a lot between individuals. These patterns are largely true for all psychiatric disorders, which led to the <a href="https://www.nimh.nih.gov/research/research-funded-by-nimh/rdoc/about-rdoc.shtml">Research Domain Criterion (RDoC)</a> framework for mapping genomics and neurological information like brain circuits to behavioural measures in order to generate novel data-driven groups. The underlying goal of the research direction is to understand the brain-behaviour relationships across diseases, to potentially identify neurologically-distinct subtypes of each disease that might be more indicative of the appropriate treatment than the diagnostic label.</p> <p style="text-align: center;"><img src="/assets/img/biotype/rdoc.png" alt="RDoC Approach"/></p> <p>In my final year with the team, I ran a preliminary study using the data to evaluate the relationships between the fMRI data collected during the study and the behavioural scores. We focused on social processing scores because they are strong predictors of “functional outcomes”: how well a schizophrenia patient will recover with treatment. Here, social processing includes things like understanding the emotional state or intentions of someone else, things we call <a href="https://www.nature.com/articles/pr92011100">theory of mind</a> or <a href="https://www.sciencedirect.com/science/article/pii/S2352154617301031">empathy</a>.</p> <p>Psychiatric studies are normally conducted by comparing cases (those who meet some DSM criteria) with controls (those who are “healthy”, i.e., don’t meet any DSM criteria). These case-control studies will compare two groups to determine the features that are somehow unique to a disorder. However, we knew that many individuals with schizophrenia have social processing that are similar to healthy controls, and that simultaneously this deficit predicted poor treatment outcomes. This study sought to find subtypes in our sample <em>regardless of diagnosis</em>, by pooling our patients and controls and finding natural groupings of the data based on features derived from both brain connectivity estimates and behavioural scores. We called these subtypes “biotypes”.</p> <p>The method employed was straightforward. Due to the small sample sizes, we decided to learn a low-dimensional representation of both the brain connectivity data and the behavioural scores using principal components analysis, and then found pairs of components that were strongly correlated across patients (this approach is known as <a href="https://online.stat.psu.edu/stat505/book/export/html/682">canonical correlation analysis</a>). These correlated components, which we can call “canonical variates”, were only retained if they were robust in the training set, which we evaluated using cross-validation. The robust canonical variates were finally clustered to generate groups with distinct neuro-behavioural profiles, regardless of diagnosis.</p> <p>The results were quite striking. A majority of the subjects formed a “normally performing group”, comprised of about almost all of the healthy controls and about half of the patients with schizophrenia. In this group, the behavioural and functional outcome scores of interest were high <em>regardless of diagnosis</em>. The rest of the patients formed a “poorly performing biotype”, and performed significantly worse on these behavioural and functional outcome tests. The difference in these functional outcome scores using a case-control comparison was much smaller than observed when forming the groups according to our biotypes, even though the functional outcome scores were not used at all to derive these new groups.</p> <p style="text-align: center;"><img src="/assets/img/biotype/scores.png" alt="Biotype Scores"/></p> <p>What was also quite interesting was the neurological differences between these groups. The regions in red (below) are regions with <em>heightened</em> connectivity in the normally-performing biotype, including many subcortical regions with extensive connectivity with frontal regions required to understand others intentions. On the other hand, the blue regions are areas with heightened connectivity in the poorly-performing biotype: these regions are primarily considered early-stage visual processing regions, with less importance in high-order cognitive processes such as understanding the intent of others. A speculative, but interesting, reason for all of this might be that schizophrenia arises from <a href="https://www.nature.com/articles/nature16549">synaptic pruning gone haywire</a>. It is normal and healthy for the brain to remove connections as it develops so that only the most useful and efficient ones survive, and if this process is abnormal, it could lead to abnormal functional connectivity profiles. Crucially, this process might happen differently for each patient afflicted with schizophrenia, and this might leave behind a unique signature that can be used to determine the ideal treatment for each patient.</p> <p style="text-align: center;"><img src="/assets/img/biotype/biotypes.png" alt="Biotype Connectivity"/></p> <p>While our study raises just as many questions as it answers, I do think that the general framework is a useful path forward for transitioning away from the DSM, and I am proud of that contribution. You can read <a href="https://www.sciencedirect.com/science/article/abs/pii/S0006322318314392">the whole paper here</a>.</p>]]></content><author><name>josephdviviano</name></author><category term="Publications"/><summary type="html"><![CDATA[Machine learning can be used to find neurologically-distinct subtypes of Schizophrenia.]]></summary></entry><entry><title type="html">Deep Learning to Detect Early Cognitive Decline</title><link href="https://www.viviano.ca/blog/2018/trajectories/" rel="alternate" type="text/html" title="Deep Learning to Detect Early Cognitive Decline"/><published>2018-01-11T10:32:20+00:00</published><updated>2018-01-11T10:32:20+00:00</updated><id>https://www.viviano.ca/blog/2018/trajectories</id><content type="html" xml:base="https://www.viviano.ca/blog/2018/trajectories/"><![CDATA[<p>Using Siamese Networks, we were able to use a cocktail of MRI scans and gene sequences to predict people who were in most need of future clinical care.</p> <p>While working at the <a href="https://www.camh.ca/">Centre for Addiction and Mental Health</a>, I had the pleasure of working with <a href="https://scholar.google.com/citations?user=SoMyf3YAAAAJ&amp;hl=en&amp;oi=ao">Jon Pipitone</a>, a computer scientist and climate activist trained at the University of Toronto, and <a href="https://dblp.org/pid/67/8968.html">Nikhil Bhagwat</a>, a defense contractor-turned-biomedical researcher, during my day job keeping the lights on at the <a href="http://imaging-genetics.camh.ca/">Kimel-Family Translational Imaging-Genetics Lab (TIGRLAB)</a>, headed by the now-VP of CAMH research, <a href="https://www.camh.ca/en/science-and-research/science-and-research-staff-directory/aristotlevoineskos">Dr. Aristotle Voineskos</a>. In the field, there was a growing consensus that machine learning was a key to improving psychatric diagnosis and treatment, due to the high variability of outcomes given similar diagnosis using traditional tools like the Diagnostics and Statistical Manual of Mental Disorders (<a href="https://www.psychiatry.org/psychiatrists/practice/dsm">DSM</a>).</p> <p style="text-align: center;"><img src="/assets/img/trajectories/alz-progression.jpg" alt="Alzheimer's Disease Progression"/></p> <p>The specific problem we wanted to tackle was the detection of the onset of cognitive decline leading to Alzheimer’s disease, using the ADNI <a href="http://adni.loni.usc.edu/">Alzheimer’s Disease Neuroimaging Initiative</a> dataset. Being able to do this can be pretty impactful: no treatment can currently reduce the course of Alzheimer’s progression, but many treatments can delay the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6935598/">progression of the disease if it is caught early</a>. This dataset contains MRI scans, SNP hits from blood draws, and cognitive scores over multiple timepoints for many older individual. The cognitive scores allowed us to identify multiple “cognitive trajectories” in the dataset by clustering the two subscales: the “Mini Mental State Exam (MMSE)” and the “Alzheimer’s Disease Assessment Scale” (ADAS-13). From both scales, we found a subset of individuals that did not experience substantial decline. Using MMSE, we also found a distinct subset that experienced decline, and for ADAS-13, we found two decliner groups, with one group declining more rapidly.</p> <p style="text-align: center;"><img src="/assets/img/trajectories/trajectories.png" alt="Identified Trajectories"/></p> <p>Having defined our groups, we then calculated cortical thickness measures from the MRI scans. This measures the thickness of the grey matter across the cortical sheet of the brain: the thickness of the cortex at different regions of the cortex is <a href="https://www.pnas.org/content/110/4/1488">associated with differing densities of glial in those regions</a>, the <a href="https://www.cell.com/neuron/fulltext/S0896-6273(18)30956-5">ability of the region to integrate signals to be set to other regions</a>, is known to vary across individuals with different neurological disorders, and has high test-retest reliability due to the stability of the signal (unlike fMRI, which is much more susceptible to the patient’s cognitive state and other sources of noise).</p> <p style="text-align: center;"><img src="/assets/img/trajectories/thickness.png" alt="Cortical Thickness"/></p> <p>Since we wanted to predict decline, we fed pairs of images from adjacent timepoints into a Siamese network, which allowed us to learn a distance embedding that represented the difference between the brain scans over time. Siamese networks are cool because they allow you to input two images of the same modality to calculate some kind of “task-dependent difference metric”. This is far less fragile than calculating the differences between the data in the input space. We incorporated genetic information into the prediction by multiplying this embedding by the APOE4 status of the individual: this gene variant is associated with the <a href="https://www.alz.org/alzheimers-dementia/what-is-alzheimers/causes-and-risk-factors/genetics">development of Alzheimer’s disease and disease progression at an earlier age</a>.</p> <p style="text-align: center;"><img src="/assets/img/trajectories/overview.png" alt="Model Overview"/></p> <p>The results of the study were clear: adding a second time-point made trajectory prediction far easier for all models, and the use of Siamese networks to build a distance embedding outperformed all other baselines tested. I’m proud of this work because, while it didn’t produce a model that was immediately clinically useful, we laid the groundwork for combining multiple clinically-meaningful biological signals with a longitudinal framework for modeling disease progression, and I believe this general direction will lead to useful future diagnostics (especially if we can get the cost of an MRI scan down to the level where routine screening would be feasible).</p> <p>For all of the details, see <a href="https://journals.plos.org/ploscompbiol/article?rev=2&amp;id=10.1371/journal.pcbi.1006376">the full paper here.</a>.</p>]]></content><author><name>josephdviviano</name></author><category term="Publications"/><summary type="html"><![CDATA[Using Siamese Networks, we were able to use a cocktail of MRI scans and gene sequences to predict people who were in most need of future clinical care.]]></summary></entry><entry><title type="html">Brain Networks Becomes More Disorganized with Age</title><link href="https://www.viviano.ca/blog/2016/aging/" rel="alternate" type="text/html" title="Brain Networks Becomes More Disorganized with Age"/><published>2016-01-09T10:32:20+00:00</published><updated>2016-01-09T10:32:20+00:00</updated><id>https://www.viviano.ca/blog/2016/aging</id><content type="html" xml:base="https://www.viviano.ca/blog/2016/aging/"><![CDATA[<p>Sometimes, it’s easier to understand how a complex system works by zooming out and thinking less about the details.</p> <p>For decades, functional MRI (fMRI) researchers primarily looked for “task evoked” activity in the brain by looking for small deviations of the BOLD signal against what appeared to be a massive wall of background “noise”. The signal to noise ratio (SNR) of the task-evoked BOLD signal is small: only a <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/1522-2594%28200012%2944%3A6%3C925%3A%3AAID-MRM14%3E3.0.CO%3B2-M">few percentage points at 1.5 T</a>, which is a major contributor to the <a href="https://www.nature.com/articles/s42003-018-0073-z">reproducibility crisis in fMRI studies</a>.</p> <p>In 1995, researchers discovered that areas of the brain’s primary motor cortex seemed to have highly correlated BOLD signals with other regions known to be involved with motor output <em>even when the person in the scanner was doing nothing in particular</em>: they called this correlation analysis “<a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.1910340409">functional connectivity</a>”. At first, this “resting state” signal was met with a lot of skepticism in the community because, for many, a neurological phenomenon without a corresponding behavioral one is uninteresting noise. The research over the next two decades would find irrefutable evidence that <a href="https://www.pnas.org/content/103/37/13848.short">the resting state signal is highly reproducible</a> and that <a href="https://www.pnas.org/content/106/31/13040.short">they form networks that have a tight correspondence with the regions identified in the task-based fMRI literature</a> … without the issues related to reproducibility. To me, this was extremely exciting: the fMRI signal is both <em>psychologically relevant</em> and <em>reproducible</em> making it a signal from which to derive a meaningful <em>psychiatric biomarker</em>.</p> <p style="text-align: center;"><img src="/assets/img/rest/rest-task.png" alt="Resting State Networks"/></p> <blockquote> <p>RSNs and task networks (left side and right side of each panel, respectively) using <a href="http://www.humanconnectomeproject.org/">HCP data</a> that correspond to the ten rest/task networks reported in <a href="https://www.pnas.org/content/106/31/13040.short">Smith et al</a>. Networks are shown in red-yellow(+)/blue-light blue(−), thresholded Z = 3.0. Networks are: (1) medial visual, (2) occipital pole, (3) lateral occipital, (4) default mode, (5) cerebellum, (6) sensorimotor, (7) auditory, (8) executive control, (9) right frontoparietal, and (10) left frontoparietal. Taken from <a href="https://www.nature.com/articles/s41598-018-35209-6">this paper</a>.</p> </blockquote> <p>The first project I got involved in following this general direction was looking for a biomarker of cognitive decline in aging. In the brain, the “default mode” network is often associated with introspection, or the maintenance of the a gestalt representation of your own place within a broader context (both spatially and temporally). <a href="https://pubmed.ncbi.nlm.nih.gov/27424918/">This paper</a> is the best demonstration I have seen of this fact: <a href="http://www.honeylab.org/">Christopher Honey</a> showed people narratives scrambled at the word, sentence, and paragraph level, and found that the default mode network comes more into play the longer the coherent temporal sequence shown to the participants. In contract the “executive control” network is associated with planning, action, and inhibiting impulses: it is associated with the kind of long-term planning that is responsible for the anthropocene, and its malfunction is associated with <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010945215003068">attention impairments</a> and <a href="https://pubmed.ncbi.nlm.nih.gov/24583406/">cognitive decline in old age</a>. Interestingly, these two networks counteract one another, which was causally shown using (transcranial magnetic stimulation)[https://www.pnas.org/content/110/49/19944], which led some to believe that this push-pull mechanism between the executive control network and the default mode network was important for proper executive function.</p> <p>The work I was involved in collected resting state and task-derived BOLD signals from older and younger participants to see whether there was a systematic difference in the functional connectivity between a set of default mode and executive control regions. The main result is presented below.</p> <p style="text-align: center;"><img src="/assets/img/rest/old-vs-young.png" alt="Old Brains, Crossed Wires"/></p> <p>The results were unequivocal: older patients showed significant crosstalk between the two networks, whereas in younger people the push-pull dynamic was intact. It was the first time I used the resting state BOLD signal to find a biomarker of a psychologically-relevant trait.</p> <p>Here’s <a href="https://www.sciencedirect.com/science/article/abs/pii/S0197458016300896">the paper</a> for more details.</p>]]></content><author><name>josephdviviano</name></author><category term="Publications"/><summary type="html"><![CDATA[Sometimes, it’s easier to understand how a complex system works by zooming out and thinking less about the details.]]></summary></entry><entry><title type="html">How I Took a MRI of a Very Small Part of my own Brain</title><link href="https://www.viviano.ca/blog/2015/trn/" rel="alternate" type="text/html" title="How I Took a MRI of a Very Small Part of my own Brain"/><published>2015-02-04T10:32:20+00:00</published><updated>2015-02-04T10:32:20+00:00</updated><id>https://www.viviano.ca/blog/2015/trn</id><content type="html" xml:base="https://www.viviano.ca/blog/2015/trn/"><![CDATA[<p>I was the first person ever to record a fMRI signal from the thalamic reticular nucleus.</p> <p>The TRN is small inhibitory structure in the brain that is implicated in <a href="https://www.quantamagazine.org/to-pay-attention-the-brain-uses-filters-not-a-spotlight-20190924/">attentional filtering</a>. My boss at the time was the Director of <a href="https://mri.info.yorku.ca/">York University’s MRI centre</a>, <a href="https://www.psych.udel.edu/people/full-list-searchable/kschneider?uid=kschneider&amp;Name=Keith%20Schneider">Keith Schneider</a> (and has since moved on to do the same and the University of Delaware), was mostly interested in how the <a href="https://www.jneurosci.org/content/31/23/8643.short">subcortical regions of the brain contribute to attention</a>: our proclivity to devote a lot of computational resources to only a few aspects of the data our brain could potentially access. Francis Crick (yeah, the DNA guy) was <a href="https://www.newscientist.com/article/dn14869-crick-was-right-about-vision-filter-in-the-brain/">the first person suggest that the thalamus would implement these filters</a> to perform attentional filtering, and this was <a href="https://www.nature.com/articles/nature07382">confirmed many years later</a> in experiments.</p> <p>My job was to image this attentional filter in action by recording from the TRN in a person using functional MRI (fMRI). fMRI measures blood flow in the brain (the BOLD signal), in response to neural activity (when I was doing my degree, we still didn’t fully understand the relationship between neural activity and the BOLD signal, but now <a href="https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2019.0630">we much better understand it</a>). The big problem with fMRI is that the signal to noise ratio is low, has a lot of geometric distortions as you crank up the resolution, and it is very susceptible to artifacts caused by movements. So we couldn’t just collect a brain scan, we had to collect a lot of very well-controlled, high resolution, noisy images and synthesize them into a functional picture of the TRN the same way <a href="https://keithwiley.com/astroPhotography/imageStacking.shtml">astronomers take pictures of deep space</a>.</p> <p>There were two big challenges: first, if we wanted to average many events together, we would need a really good idea of a particular event type that would drive TRN activity. We considered an attention switching task, but I found a lot of research that suggested different neural populations have different resonant frequencies, and that those resonant frequencies could be driven by external periodic stimuli, shown both in <a href="https://pubmed.ncbi.nlm.nih.gov/11355381/">humans</a> and <a href="https://journals.physiology.org/doi/full/10.1152/jn.00388.2011">animals</a>.</p> <p>To maximize the chance I would find the TRN, I didn’t want to rely on human behavior: I decided I just wanted to drive the human retina the same way animal researchers drop an electrode directly into the brain to zap it and record the response. I wanted a regular stimulus that could be down to people over and over again, that would give me a decent response. To do this, I started with something called “retinotopic mapping”. The primary visual system in the brain is organized like a reverse projector: different regions of the space in front of you map to specific locations on the cortex and thalamus. They call this the “retinotopic map”.</p> <p style="text-align: center;"><img src="/assets/img/msc/retinotopic-drawing.gif" alt="Retinotopy"/></p> <p>In fact, you can even project simple shapes onto the cortex and read them out using fMRI. A very expensive party trick. People map the “retinotopy” using a spinning disk at a fixed speed. Since they know the speed of the disk, they can look for brain regions that activate periodically at that frequency. My idea was to superimpose a flicker modulation on top of a spinning disk that stepped through frequencies at a different rate:</p> <p style="text-align: center;"><img src="/assets/img/msc/stimulus.png" alt="Stimulus"/></p> <p>Any now for the coolest part: the retinotopic maps of my own lateral geniculate nucleus and thalamic reticular nucleus on top of a very high resolution brain scan:</p> <p style="text-align: center;"><img src="/assets/img/msc/my-brain.png" alt="My Brain"/></p> <p>The main interesting thing we found was that the thalamic reticular nucleus seemed to be inhibiting the lateral geniculate nucleus at particular flicker frequencies: around 10-12 Hz, the lateral geniculate nucleus’s activity dominates (corresponding to the “alpha frequency” which is <a href="https://www.jneurosci.org/content/37/42/10173.abstract">heavily implicated in visual attention</a>), but interestingly, the thalamic reticular nucleus’ activity seemed to be maximal in the 20 - 60 Hz range where the lateral geniculate nucleus is less active: these frequencies correspond to the “beta/gamma frequencies” and are associated with <a href="https://www.nature.com/articles/s41598-018-25267-1">top down feedback</a> from non-visual areas (<a href="https://www.jneurosci.org/content/37/28/6698">also see</a>). It seems like driving your brain with a high flicker frequency sends shock-waves through your brain and activates an automatic feedback mechanism. I believe it: strobe lights are exciting at concerts for a reason.</p> <p>This was my first big science project and I would have done a lot of things differently (including try to use an attention task to get a more interesting signal out of the brain). Aside from teaching me a tonne about physics and how the brain operates as distributed network, this was the project that taught me the basics of programming, and that kick-started my career.</p> <p>If you want all of the dry details, <a href="https://www.jneurosci.org/content/35/5/2026.abstract">here’s the paper</a>.</p>]]></content><author><name>josephdviviano</name></author><category term="Publications"/><summary type="html"><![CDATA[I was the first person ever to record a fMRI signal from the thalamic reticular nucleus.]]></summary></entry></feed>